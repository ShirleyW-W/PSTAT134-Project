{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import warnings\n",
    " \n",
    "warnings.filterwarnings(action='ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_news = pd.read_csv(\"Fake.csv\")\n",
    "true_news = pd.read_csv(\"True.csv\")\n",
    "# add labels\n",
    "fake_news[\"label\"] = \"Fake\"\n",
    "true_news[\"label\"] = \"True\"\n",
    "\n",
    "# combine the two separate dataframes\n",
    "corpus = pd.concat([fake_news, true_news], ignore_index = True)[\"text\"]\n",
    "tokenized_corpus = [simple_preprocess(sentence) for sentence in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    Donald Trump just couldn t wish all Americans ...\n",
       "1    House Intelligence Committee Chairman Devin Nu...\n",
       "2    On Friday, it was revealed that former Milwauk...\n",
       "3    On Christmas day, Donald Trump announced that ...\n",
       "4    Pope Francis used his annual Christmas Day mes...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CBOW and Skip-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4, sg=0)\n",
    "model_sg = Word2Vec(sentences=tokenized_corpus, vector_size=100, window=5, min_count=1, workers=4, sg=1)\n",
    "\n",
    "# Parameters:\n",
    "# - vector_size: Dimensionality of word embeddings\n",
    "# - window: Context window size\n",
    "# - min_count: Ignores words appearing less than min_count times\n",
    "# - workers: Number of CPU cores to use\n",
    "# - sg: 0 for CBOW (Continuous Bag of Words), 1 for Skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_cbow.save(\"word2vec_cbow.model\")\n",
    "model_sg.save(\"word2vec_sg.model\")\n",
    "\n",
    "# Load the model\n",
    "# loaded_model = Word2Vec.load(\"word2vec.model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ShirleyW-W/PSTAT134-Project/blob/charles/BERTFakeNewsClassifier.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37726536",
      "metadata": {
        "id": "37726536"
      },
      "source": [
        "# Using a Pre-trained BERT Model for Text Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6ccc150f",
      "metadata": {
        "id": "6ccc150f"
      },
      "source": [
        "## Loading Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f4d47c51",
      "metadata": {
        "id": "f4d47c51"
      },
      "outputs": [],
      "source": [
        "#!pip install numpy==1.23.5\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0b167936",
      "metadata": {
        "id": "0b167936"
      },
      "source": [
        "## Import Real and Fake News Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "3c0683a4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c0683a4",
        "outputId": "4bbcd2da-0ca3-4a0e-f1f5-94feca663ca6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading from https://www.kaggle.com/api/v1/datasets/download/clmentbisaillon/fake-and-real-news-dataset?dataset_version_number=1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 41.0M/41.0M [00:02<00:00, 14.8MB/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting files...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /root/.cache/kagglehub/datasets/clmentbisaillon/fake-and-real-news-dataset/versions/1\n"
          ]
        }
      ],
      "source": [
        "# !pip install kagglehub\n",
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"clmentbisaillon/fake-and-real-news-dataset\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "091609e8",
      "metadata": {
        "id": "091609e8"
      },
      "outputs": [],
      "source": [
        "class BERTZeroShotClassifier:\n",
        "    def __init__(self, candidate_labels=None):\n",
        "        self.model_name = \"facebook/bart-large-mnli\"\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
        "        self.model = AutoModelForSequenceClassification.from_pretrained(self.model_name)\n",
        "\n",
        "        self.model.eval()\n",
        "\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "\n",
        "        # Define the candidate labels for zero-shot classification\n",
        "        if candidate_labels is None:\n",
        "            self.candidate_labels = [\n",
        "                {\"real\": [\"factual news\", \"verified information\"]},\n",
        "                {\"fake\": [\"fake news\", \"misinformation\"]}\n",
        "            ]\n",
        "        else:\n",
        "            self.candidate_labels = candidate_labels\n",
        "\n",
        "        self.all_labels = []\n",
        "        for category_dict in self.candidate_labels:\n",
        "            for category, label_list in category_dict.items():\n",
        "                for label in label_list:\n",
        "                    self.all_labels.append(label)\n",
        "\n",
        "    def classify_text(self, article_text, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Classify text using zero-shot classification with multiple label categories\n",
        "\n",
        "        Arguments:\n",
        "            article_text (str): the text of the news article\n",
        "            threshold (float): Confidence threshold for classification\n",
        "\n",
        "        Returns:\n",
        "            dict: classification results with categories, scores, and overall classification\n",
        "        \"\"\"\n",
        "\n",
        "        # prepare inputs for the model\n",
        "        inputs = self.tokenizer(\n",
        "            f\"This is a news article: {article_text}\",\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        #prepare label inputs\n",
        "        label_inputs = []\n",
        "        for label in self.all_labels:\n",
        "            label_inputs.append(f\"This news article contains {label}.\")\n",
        "\n",
        "        label_encodings = self.tokenizer(\n",
        "            label_inputs,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=128,\n",
        "            padding=True\n",
        "        )\n",
        "\n",
        "        #move inputs to device\n",
        "        inputs = {k: v.to(self.device) for k, v in inputs.items()}\n",
        "        label_encodings = {k: v.to(self.device) for k, v in label_encodings.items()}\n",
        "\n",
        "        #perform classification\n",
        "        with torch.no_grad():\n",
        "            text_embedding = self.model(**inputs).logits\n",
        "\n",
        "            #get embeddings for each label\n",
        "            label_embeddings = []\n",
        "            for i in range(len(self.all_labels)):\n",
        "                label_inputs = {\n",
        "                    'input_ids': label_encodings['input_ids'][i].unsqueeze(0),\n",
        "                    'attention_mask': label_encodings['attention_mask'][i].unsqueeze(0)\n",
        "                }\n",
        "                label_embedding = self.model(**label_inputs).logits\n",
        "                label_embeddings.append(label_embedding)\n",
        "\n",
        "            # calculate similarity scores\n",
        "            scores = []\n",
        "            for label_embedding in label_embeddings:\n",
        "                similarity = torch.nn.functional.cosine_similarity(text_embedding, label_embedding)\n",
        "                scores.append(similarity.item())\n",
        "\n",
        "\n",
        "            #normalize scores\n",
        "            scores = np.array(scores)\n",
        "            scores = (scores - scores.min()) / (scores.max() - scores.min())\n",
        "\n",
        "            results = {\n",
        "                \"label_scores\": {label: float(score) for label, score in zip(self.all_labels, scores)},\n",
        "                \"categories\": {}\n",
        "            }\n",
        "\n",
        "            label_index = 0\n",
        "            for category_dict in self.candidate_labels:\n",
        "                for category, label_list in category_dict.items():\n",
        "                    category_scores = []\n",
        "                    for _ in label_list:\n",
        "                        category_scores.append(scores[label_index])\n",
        "                        label_index += 1\n",
        "\n",
        "                        results[\"categories\"][category] = float(np.mean(category_scores))\n",
        "\n",
        "            top_category = max(results[\"categories\"].items(), key=lambda x: x[1])\n",
        "            results[\"classification\"] = top_category[0]\n",
        "            results[\"confidence\"] = top_category[1]\n",
        "            results[\"is_confident\"] = top_category[1] > threshold\n",
        "\n",
        "            if not results[\"is_confident\"]:\n",
        "              results[\"classification\"] = \"unknown\"\n",
        "\n",
        "            return results\n",
        "\n",
        "    def classify_batch(self, article_texts, threshold=0.5):\n",
        "        \"\"\"\n",
        "        Classify multiple news articles\n",
        "\n",
        "        Arguments:\n",
        "            article_texts (list): list of article text strings\n",
        "            threshold (float): confidence threshold\n",
        "\n",
        "        Returns:\n",
        "            list: list of classification results\n",
        "        \"\"\"\n",
        "        return [self.classify_text(article, threshold) for article in article_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "a8c7c508",
      "metadata": {
        "id": "a8c7c508"
      },
      "outputs": [],
      "source": [
        "def load_and_preprocess_dataset(fake_path, real_path, sample_size=None, seed=0):\n",
        "    fake_df = pd.read_csv(fake_path)\n",
        "    real_df = pd.read_csv(real_path)\n",
        "\n",
        "    # add label column\n",
        "    fake_df['label'] = 'fake'\n",
        "    real_df['label'] = 'real'\n",
        "\n",
        "    # Combine textual information of subject, title, and article text\n",
        "    fake_df['content'] = \"Subject: \" + fake_df['subject'] + \", Title: \" + fake_df['title'] + \" \" + fake_df['text']\n",
        "    real_df['content'] = \"Subject: \" + real_df['subject'] + \", Title: \" + real_df['title'] + \" \" + real_df['text']\n",
        "\n",
        "    # select relevant columns: \"article content\", \"label\"\n",
        "    fake_df = fake_df[['content', 'label']]\n",
        "    real_df = real_df[['content', 'label']]\n",
        "\n",
        "    #sample if needed\n",
        "    if sample_size:\n",
        "        fake_df = fake_df.sample(sample_size, random_state = seed)\n",
        "        real_df = real_df.sample(sample_size, random_state = seed)\n",
        "\n",
        "    # combine datasets\n",
        "    combined_df = pd.concat([fake_df, real_df], ignore_index = True)\n",
        "\n",
        "    # shuffle dataset\n",
        "    combined_df = combined_df.sample(frac=1, random_state=seed).reset_index(drop=True)\n",
        "\n",
        "    # apply parallel preprocessing for texts\n",
        "    combined_df = parallel_preprocess_dataset(combined_df)\n",
        "\n",
        "    return combined_df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text):\n",
        "  \"\"\"\n",
        "  preprocess text by performing tokenization, stop word removal, lemmatization and stemming\n",
        "\n",
        "  Arguments:\n",
        "      text (str): input text to preprocess\n",
        "\n",
        "  Returns:\n",
        "      str: preprocessed text\n",
        "  \"\"\"\n",
        "\n",
        "  from nltk.corpus import stopwords\n",
        "  from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
        "  from nltk.tokenize import word_tokenize\n",
        "  import re\n",
        "\n",
        "  text = text.lower()\n",
        "\n",
        "  text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "\n",
        "  # Remove special characters and numbers\n",
        "  text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "\n",
        "  tokens = word_tokenize(text)\n",
        "\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "\n",
        "  # lemmatization\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "\n",
        "  # stemming\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed_tokens = [stemmer.stem(word) for word in lemmatized_tokens]\n",
        "\n",
        "  preprocessed_text = ' '.join(stemmed_tokens)\n",
        "\n",
        "  return preprocessed_text"
      ],
      "metadata": {
        "id": "E8BlAOCYfMfH"
      },
      "id": "E8BlAOCYfMfH",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def parallel_preprocess_dataset(df, n_jobs=-1):\n",
        "  \"\"\"\n",
        "  Apply preprocessing to the content column of a dataframe in parallel\n",
        "\n",
        "  Arguments:\n",
        "    df (pd.DataFrame): DataFrame with 'content' column\n",
        "    n_jobs (int): Number of jobs to run in parallel. -1 means using all processors\n",
        "\n",
        "  Returns:\n",
        "    pd.DataFrame: DataFrame with preprocessed 'processed_content' column\n",
        "  \"\"\"\n",
        "\n",
        "  from joblib import Parallel, delayed\n",
        "  import nltk\n",
        "\n",
        "  try:\n",
        "      nltk.data.find('tokenizers/punkt')\n",
        "      nltk.data.find('tokenizers/punkt_tab')\n",
        "      nltk.data.find('corpora/stopwords')\n",
        "      nltk.data.find('corpora/wordnet')\n",
        "  except LookupError:\n",
        "      nltk.download('punkt')\n",
        "      nltk.download('punkt_tab')\n",
        "      nltk.download('stopwords')\n",
        "      nltk.download('wordnet')\n",
        "\n",
        "  processed_df = df.copy()\n",
        "\n",
        "  processed_texts = Parallel(n_jobs=n_jobs)(\n",
        "      delayed(preprocess_text)(text) for text in processed_df['content'].values\n",
        "  )\n",
        "\n",
        "  processed_df['processed_content'] = processed_texts\n",
        "\n",
        "  return processed_df"
      ],
      "metadata": {
        "id": "JOuwN-ObeFyv"
      },
      "id": "JOuwN-ObeFyv",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "cf56359b",
      "metadata": {
        "id": "cf56359b"
      },
      "outputs": [],
      "source": [
        "def evaluate_model(classifier, test_df, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Evaluate the classifier on the test dataset\n",
        "\n",
        "    Arguments:\n",
        "        classifier (BertZeroShotClassifier): the classifier\n",
        "        test_df (pd.DataFrame): test dataset\n",
        "\n",
        "    Returns:\n",
        "        dict: Evaluation metrics\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = []\n",
        "    confidences = []\n",
        "\n",
        "    results = classifier.classify_batch(test_df['content'].tolist(), threshold)\n",
        "    predictions = [result['classification'] for result in results]\n",
        "    confidences = [result['confidence'] for result in results]\n",
        "\n",
        "    # calculate metrics\n",
        "    accuracy = accuracy_score(test_df['label'], predictions)\n",
        "    report = classification_report(test_df['label'], predictions, output_dict=True)\n",
        "    conf_matrix = confusion_matrix(test_df['label'], predictions, labels = ['real', 'fake'])\n",
        "\n",
        "    # add predictions and confidences to result dataframe\n",
        "    results_df = test_df.copy()\n",
        "    results_df['predicted'] = predictions\n",
        "    results_df['confidence'] = confidences\n",
        "\n",
        "    # identify misclassified articles\n",
        "    results_df['misclassified'] = results_df['label'] != results_df['predicted']\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'classification_report': report,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'results_df': results_df\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "795ad06c",
      "metadata": {
        "id": "795ad06c"
      },
      "source": [
        "## Execution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "e88351ce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e88351ce",
        "outputId": "48d798f9-06d0-44fe-8e75-637e508b348b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model...\n",
            "\n",
            "Results:\n",
            "Accuracy: 0.4500\n",
            "\n",
            "Classification Report:\n",
            "              precision  recall  f1-score  support\n",
            "fake           0.470588    0.80  0.592593   100.00\n",
            "real           0.333333    0.10  0.153846   100.00\n",
            "accuracy       0.450000    0.45  0.450000     0.45\n",
            "macro avg      0.401961    0.45  0.373219   200.00\n",
            "weighted avg   0.401961    0.45  0.373219   200.00\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluating the model...\n",
            "\n",
            "Results:\n",
            "Accuracy: 0.5550\n",
            "\n",
            "Classification Report:\n",
            "              precision  recall  f1-score  support\n",
            "fake           0.554455   0.560  0.557214  100.000\n",
            "real           0.555556   0.550  0.552764  100.000\n",
            "accuracy       0.555000   0.555  0.555000    0.555\n",
            "macro avg      0.555006   0.555  0.554989  200.000\n",
            "weighted avg   0.555006   0.555  0.554989  200.000\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    fake_news_path = path + \"/Fake.csv\"\n",
        "    real_news_path = path + \"/True.csv\"\n",
        "\n",
        "    dataset = load_and_preprocess_dataset(fake_news_path, real_news_path, sample_size = 100, seed = 45)\n",
        "\n",
        "    # create customized labels\n",
        "    candidate_labels = [\n",
        "        {\"real\": [\"credible reporting\", \"fact-checked information\", \"objective journalism\", \"satire\"]},\n",
        "        {\"fake\": [\"misinformation\"]}\n",
        "    ]\n",
        "    # initialize the classifier\n",
        "    classifier = BERTZeroShotClassifier(candidate_labels=candidate_labels)\n",
        "\n",
        "    # evaluate the model\n",
        "    print(\"Evaluating the model...\")\n",
        "    eval_results = evaluate_model(classifier, dataset, threshold=0.3)\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Accuracy: {eval_results['accuracy']:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(pd.DataFrame(eval_results['classification_report']).T)\n",
        "\n",
        "    \"\"\"\n",
        "      classifying with new, more comprehensive candidate labels\n",
        "    \"\"\"\n",
        "\n",
        "    dataset = load_and_preprocess_dataset(fake_news_path, real_news_path, sample_size = 100, seed = 45)\n",
        "\n",
        "    # create customized labels\n",
        "    expanded_candidate_labels = [\n",
        "        {\n",
        "            \"real\": [\n",
        "                \"factual or trustworthy news\",\n",
        "                \"verified or authenticated information\",\n",
        "                \"accurate or professional reporting\",\n",
        "                \"credible, ethical, or objective journalism\",\n",
        "                \"evidence-based or unbiased reporting\",\n",
        "                \"fact-checked content\",\n",
        "                \"reliable or moderate sources\",\n",
        "                \"substantiated claims\",\n",
        "                \"balanced coverage\"\n",
        "            ]\n",
        "        },\n",
        "        {\n",
        "            \"fake\": [\n",
        "                \"fake news\",\n",
        "                \"misinformation\",\n",
        "                \"false reporting\",\n",
        "                \"fabricated content\",\n",
        "                \"misleading information\",\n",
        "                \"unverified claims\",\n",
        "                \"propaganda\",\n",
        "                \"deceptive content\",\n",
        "                \"clickbait\",\n",
        "                \"hoax\",\n",
        "                \"disinformation\",\n",
        "                \"manipulated facts\",\n",
        "                \"sensationalism\",\n",
        "                \"conspiracy theory\",\n",
        "                \"rumor\"\n",
        "            ]\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # initialize the classifier\n",
        "    classifier = BERTZeroShotClassifier(candidate_labels=expanded_candidate_labels)\n",
        "\n",
        "    # evaluate the model\n",
        "    print(\"Evaluating the model...\")\n",
        "    eval_results = evaluate_model(classifier, dataset, threshold=0.3)\n",
        "\n",
        "    print(\"\\nResults:\")\n",
        "    print(f\"Accuracy: {eval_results['accuracy']:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(pd.DataFrame(eval_results['classification_report']).T)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}